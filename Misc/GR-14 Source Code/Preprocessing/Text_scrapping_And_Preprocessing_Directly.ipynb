{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "path = 'C:\\\\Program Files (x86)\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe'\n",
    "chrome_options=webdriver.ChromeOptions()\n",
    "chrome_options.binary_location=path\n",
    "\n",
    "driver=webdriver.Chrome(options=chrome_options)\n",
    "driver.get('https://www.facebook.com/login')\n",
    "\n",
    "username_field=driver.find_element('id','email')\n",
    "password_field=driver.find_element('id','pass')\n",
    "username_field.send_keys(\"shrihaari12@gmail.com\")\n",
    "password_field.send_keys(\"Kag97_Rez#\")\n",
    "\n",
    "password_field.send_keys(Keys.RETURN)\n",
    "time.sleep(2)\n",
    "\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "#profile_url='https://www.facebook.com/sanjeev.pratap.186'\n",
    "profile_url = 'https://www.facebook.com/animesh.paloi.7?mibextid=ZbWKwL'\n",
    "driver.get(profile_url)\n",
    "time.sleep(2)\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989\n",
      "6391     1989\n",
      "6391\n",
      "9737     6391\n",
      "9737\n",
      "13429     9737\n",
      "13429\n",
      "16810     13429\n",
      "16810\n",
      "20872     16810\n",
      "20872\n",
      "25143     20872\n",
      "25143\n",
      "26651     25143\n",
      "26651\n",
      "Timeout reached. Page loading took too long.\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "Page loading took too long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8676\\1780416723.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Timeout reached. Page loading took too long.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Page loading took too long\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Wait for 1 second before checking again\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: Page loading took too long"
     ]
    }
   ],
   "source": [
    "html_list = []\n",
    "timeout = 60  # Define a timeout value (in seconds)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        \n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        print(last_height)\n",
    "        start_time = time.time()  # Record the start time\n",
    "        \n",
    "        #wait = WebDriverWait(driver, 10)\n",
    "        #prefs = {\"profile.default_content_setting_values.notifications\" : 2}\n",
    "        #chrome_options.add_experimental_option(\"prefs\",prefs)\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "        time.sleep(5)\n",
    "        # Wait for the page to load fully\n",
    "        while True:\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height != last_height:\n",
    "                break\n",
    "            elif time.time() - start_time > timeout:\n",
    "                print(\"Timeout reached. Page loading took too long.\")\n",
    "                raise TimeoutError(\"Page loading took too long\")\n",
    "            else:\n",
    "                time.sleep(1)  # Wait for 1 second before checking again\n",
    "\n",
    "        # Get the HTML content of the page\n",
    "        html_content = driver.page_source\n",
    "        html_list.append(html_content)\n",
    "\n",
    "        print(new_height, \"   \", last_height)\n",
    "\n",
    "        if new_height == last_height:\n",
    "            print(\"Reached the end of the page\")\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Scrolling interrupted by user\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "slist=[]\n",
    "for page_source in html_list:\n",
    "    \n",
    "    soup=BeautifulSoup(page_source,'html.parser')\n",
    "    slist.append(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(slist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = 168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translator(corpus):\n",
    "    translated_corpus = []\n",
    "    #print(corpus)\n",
    "    for text in corpus:\n",
    "        #print(text)\n",
    "        if text is not None and text.strip():  # Check if text is not empty\n",
    "            translated_text=GoogleTranslator(source='auto', target='en').translate(text)\n",
    "            translated_corpus.append(translated_text)\n",
    "        else:\n",
    "            translated_corpus.append(\"\")  # If no text provided, add empty string\n",
    "    return translated_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL TEXT:  Up in the hills#travelling#hikingadventures\n",
      "TRANSLATED TEXT:  Up in the hills#travelling#hikingadventures\n",
      "ORIGINAL TEXT:  Diwali  Celebration \n",
      "TRANSLATED TEXT:  Diwali  Celebration\n",
      "ORIGINAL TEXT:  Writting story online for MAR activity\n",
      "TRANSLATED TEXT:  Writting story online for MAR activity\n",
      "ORIGINAL TEXT:  Technical Review By Animesh Paloi\n",
      "TRANSLATED TEXT:  Technical Review By Animesh Paloi\n",
      "ORIGINAL TEXT:  আনলিমিটেড ঝগড়া + আনলিমিটেডকেয়ারিং+ আনলিমিটেড ভালোবাসা= দুই ভাইবোন\n",
      "TRANSLATED TEXT:  Unlimited Quarrels + Unlimited Caring + Unlimited Love = Two Siblings\n",
      "ORIGINAL TEXT:  ~:মুখেভাত:~®দাদাই ডাকে “দাদান” সোনা,ঠাম্মি ডাকে “শ্রী”মাম্মি-পাপার “বেবো” আমি… See more\n",
      "TRANSLATED TEXT:  ~:Mukhevat:~®Grandpa calls “Dadaan” Gold, Thammi calls “Sri” Mom-Papers “Babo” I… See more\n",
      "ORIGINAL TEXT:  HAPPY INDEPENDENT DAY............ I LOVE MY INDIA\n",
      "TRANSLATED TEXT:  HAPPY INDEPENDENT DAY............ I LOVE MY INDIA\n",
      "POST TEXT :\n",
      "{'Writting story online for MAR activity', 'Technical Review By Animesh Paloi', 'HAPPY INDEPENDENT DAY............ I LOVE MY INDIA', 'Up in the hills#travelling#hikingadventures', 'Diwali  Celebration ', '~:মুখেভাত:~®দাদাই ডাকে “দাদান” সোনা,ঠাম্মি ডাকে “শ্রী”মাম্মি-পাপার “বেবো” আমি… See more', 'আনলিমিটেড ঝগড়া + আনলিমিটেডকেয়ারিং+ আনলিমিটেড ভালোবাসা= দুই ভাইবোন'}\n",
      "TRANSLATED POST TEXT :\n",
      "{'Writting story online for MAR activity', '~:Mukhevat:~®Grandpa calls “Dadaan” Gold, Thammi calls “Sri” Mom-Papers “Babo” I… See more', 'Technical Review By Animesh Paloi', 'HAPPY INDEPENDENT DAY............ I LOVE MY INDIA', 'Up in the hills#travelling#hikingadventures', 'Unlimited Quarrels + Unlimited Caring + Unlimited Love = Two Siblings', 'Diwali  Celebration'}\n"
     ]
    }
   ],
   "source": [
    "ts=set()\n",
    "ps=set()\n",
    "for s in slist:\n",
    "\n",
    "    #post data\n",
    "    #tlist=[]\n",
    "    post_details = s.find_all(\"div\", {\"class\": \"x1iorvi4 x1pi30zi x1l90r2v x1swvt13\"})\n",
    "    for post in post_details:\n",
    "        tlist=[]\n",
    "        #original_text = re.sub(r'\\s+', ' ', post.text) #replacing all types of more than one space with single space\n",
    "        original_text = post.text  # removing the trailing spaces\n",
    "        if original_text in ps:\n",
    "            continue\n",
    "        print(\"ORIGINAL TEXT: \",original_text)\n",
    "        ps.add(original_text)\n",
    "        tlist.append(original_text)\n",
    "        tlist=translator(tlist)\n",
    "        translated_text=tlist[0]   \n",
    "        print(\"TRANSLATED TEXT: \",translated_text)\n",
    "        ts.add(translated_text)\n",
    "    \n",
    "print(\"POST TEXT :\")\n",
    "print(ps) \n",
    "print(\"TRANSLATED POST TEXT :\")\n",
    "print(ts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL TEXT:  Very smart\n",
      "ORIGINAL TEXT:  Nice\n",
      "ORIGINAL TEXT:  Mui papun\n",
      "ORIGINAL TEXT:  So nice\n",
      "ORIGINAL TEXT:  Tike has\n",
      "ORIGINAL TEXT:  super pic\n",
      "Comment TEXT : \n",
      "{'Nice', 'super pic', 'Very smart', 'Mui papun', 'Tike has', 'So nice'}\n",
      "Translated Comment TEXT: \n",
      "{'Nice', 'super pic', 'Mui Papun', 'Very smart', 'Tike has', 'So nice'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ts1=set()\n",
    "cs=set()\n",
    "\n",
    "#bs=set()\n",
    "\n",
    "#comment data\n",
    "#tlist1=[]  #NEEDS TO BE CHANGED IN OTHER DOCUMENTS\n",
    "post_details = s.find_all(\"div\", {\"class\": \"x1lliihq xjkvuk6 x1iorvi4\"})\n",
    "for post in post_details:\n",
    "    tlist1=[]\n",
    "    #original_text = re.sub(r'\\s+', ' ', post.text)\n",
    "    original_text = post.text\n",
    "    #print(\"ORIGINAL TEXT: \",original_text)\n",
    "    if original_text in cs:\n",
    "        continue\n",
    "    print(\"ORIGINAL TEXT: \",original_text)\n",
    "    cs.add(original_text)\n",
    "    tlist1.append(original_text)\n",
    "    tlist1=translator(tlist1)\n",
    "    translated_text=tlist1[0]\n",
    "    ts1.add(translated_text)\n",
    "    '''\n",
    "    #bio data\n",
    "    post_details = s.find_all(\"div\", {\"class\": \"xyamay9 x1pi30zi x1l90r2v x1swvt13\"})\n",
    "    for post in post_details:\n",
    "        original_text = re.sub(r'\\s+', ' ', post.text)\n",
    "        if original_text in bs:\n",
    "            continue\n",
    "        bs.add(original_text)\n",
    "    '''\n",
    "print(\"Comment TEXT : \")\n",
    "print(cs)\n",
    "print(\"Translated Comment TEXT: \")\n",
    "print(ts1)\n",
    "#print(\"Bio TEXT : \")\n",
    "#print(bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving everything in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 1: Full Name\n",
      "Column 2: Facebook Profile Link\n",
      "Column 3: sEXT\n",
      "Column 4: sAGR\n",
      "Column 5: sCON\n",
      "Column 6: sNEU\n",
      "Column 7: sOPN\n",
      "Column 8: PostsImage\n",
      "Column 9: PostsImage(Face)\n",
      "Column 10: PostsImage(NonFace)\n",
      "Column 11: PostsImageWithText\n",
      "Column 12: PostsImageText\n",
      "Column 13: CaptionedPostsImage\n",
      "Column 14: PostsImageCaptions\n",
      "Column 15: ProfileImages\n",
      "Column 16: ProfileImages(Face)\n",
      "Column 17: ProfileImages(NonFace)\n",
      "Column 18: ProfileImagesWithText\n",
      "Column 19: ProfileImagesText\n",
      "Column 20: CaptionedProfileImages\n",
      "Column 21: ProfileImagesCaptions\n",
      "Column 22: OriginalPosts\n",
      "Column 23: PreprocessedPosts\n",
      "Column 24: OriginalComments\n",
      "Column 25: PreprocessedComments\n",
      "Column 26: Hashtags\n",
      "Column 27: CountPostsImage(Face)\n",
      "Column 28: CountPostsImage(NonFace)\n",
      "Column 29: CountProfileImage(Face)\n",
      "Column 30: CountProfileImage(NonFace)\n",
      "Column 31: CountPostsText\n",
      "Column 32: CountComments\n",
      "Column 33: CountPostImgText\n",
      "Column 34: CountProfileImgText\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Open the CSV file in read mode\n",
    "with open('Final_Curated_Dataset1.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "    # Create a CSV reader\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    \n",
    "    # Read the first row to get the column headers\n",
    "    headers = next(csvreader)\n",
    "    \n",
    "    # Print the column headers with their corresponding numbering\n",
    "    for i, header in enumerate(headers, start=1):\n",
    "        print(f\"Column {i}: {header}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "# Define the row indices to update\n",
    "#row_index = 39 \n",
    "\n",
    "OrigPostText_str = ' '.join(ps)\n",
    "OrigComment_str = ' '.join(cs)\n",
    "\n",
    "import csv\n",
    "\n",
    "# Read the CSV file and store its contents in a list of lists\n",
    "with open('Final_Curated_Dataset1.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    data = list(csvreader)\n",
    "\n",
    "# Update the value in memory\n",
    "data[row_index - 1][21] = OrigPostText_str\n",
    "data[row_index - 1][23] = OrigComment_str\n",
    "data[row_index - 1][30] = len(ps)\n",
    "data[row_index - 1][31] = len(cs)\n",
    "\n",
    "# Write the modified data back to the CSV file\n",
    "with open('Final_Curated_Dataset1.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerows(data)\n",
    "#print(data)\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "with open('Final_Curated_Dataset1.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    data = list(csvreader)\n",
    "\n",
    "var_orig_post=data[row_index - 1][21]\n",
    "var_orig_comment=data[row_index - 1][23]\n",
    "\n",
    "print(type(var_orig_post))\n",
    "print(type(var_orig_comment))\n",
    "\n",
    "words_orig_post = var_orig_post.split(' ')\n",
    "print(type(words_orig_post))\n",
    "\n",
    "words_orig_comment = var_orig_comment.split(' ')\n",
    "print(type(words_orig_comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writting story online for MAR activity Technical Review By Animesh Paloi HAPPY INDEPENDENT DAY............ I LOVE MY INDIA Up in the hills#travelling#hikingadventures Diwali  Celebration  ~:মুখেভাত:~®দাদাই ডাকে “দাদান” সোনা,ঠাম্মি ডাকে “শ্রী”মাম্মি-পাপার “বেবো” আমি… See more আনলিমিটেড ঝগড়া + আনলিমিটেডকেয়ারিং+ আনলিমিটেড ভালোবাসা= দুই ভাইবোন\n"
     ]
    }
   ],
   "source": [
    "print(var_orig_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice super pic Very smart Mui papun Tike has So nice\n"
     ]
    }
   ],
   "source": [
    "print(var_orig_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", sample)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    #words=words.split()\n",
    "    #new_words=[]\n",
    "    #for word in words:\n",
    "    new_words = re.sub(r'[^\\w\\s]', ' ', words)\n",
    "    new_words = re.sub(r'_', ' ', new_words)\n",
    "        #if new_word != '':\n",
    "            #new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_numbers(words):\n",
    "    \"\"\"Remove all interger occurrences in list of tokenized words\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            continue\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_alphanumeric(words):\n",
    "    \"\"\"Remove all interger+alphabet occurrences in list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'\\b\\w*\\d\\w*\\b', '', word)\n",
    "        if new_word!= '':                                #12abc\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            continue\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Writting story online for MAR activity', '~:Mukhevat:~®Grandpa calls “Dadaan” Gold, Thammi calls “Sri” Mom-Papers “Babo” I… See more', 'Technical Review By Animesh Paloi', 'HAPPY INDEPENDENT DAY............ I LOVE MY INDIA', 'Up in the hills#travelling#hikingadventures', 'Unlimited Quarrels + Unlimited Caring + Unlimited Love = Two Siblings', 'Diwali  Celebration'}\n"
     ]
    }
   ],
   "source": [
    "print(ts) #translated post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writting story online for MAR activity ~:Mukhevat:~®Grandpa calls “Dadaan” Gold, Thammi calls “Sri” Mom-Papers “Babo” I… See more Technical Review By Animesh Paloi HAPPY INDEPENDENT DAY............ I LOVE MY INDIA Up in the hills#travelling#hikingadventures Unlimited Quarrels + Unlimited Caring + Unlimited Love = Two Siblings Diwali  Celebration\n"
     ]
    }
   ],
   "source": [
    "trans_post_str = ' '.join([str(item) for item in ts if item is not None])\n",
    "print(trans_post_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Writting story online for MAR activity ~:Mukhevat:~®Grandpa calls “Dadaan” Gold, Thammi calls “Sri” Mom-Papers “Babo” I… See more Technical Review By Animesh Paloi HAPPY INDEPENDENT DAY............ I LOVE MY INDIA Up in the hills#travelling#hikingadventures Unlimited Quarrels + Unlimited Caring + Unlimited Love = Two Siblings Diwali  Celebration'"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_post_str=remove_URL(trans_post_str)\n",
    "trans_post_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Writting story online for MAR activity ~:Mukhevat:~®Grandpa calls “Dadaan” Gold, Thammi calls “Sri” Mom-Papers “Babo” I… See more Technical Review By Animesh Paloi HAPPY INDEPENDENT DAY............ I LOVE MY INDIA Up in the hills#travelling#hikingadventures Unlimited Quarrels + Unlimited Caring + Unlimited Love = Two Siblings Diwali  Celebration'"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_post_str=replace_contractions(trans_post_str)\n",
    "trans_post_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Writting story online for MAR activity   Mukhevat   Grandpa calls  Dadaan  Gold  Thammi calls  Sri  Mom Papers  Babo  I  See more Technical Review By Animesh Paloi HAPPY INDEPENDENT DAY             I LOVE MY INDIA Up in the hills travelling hikingadventures Unlimited Quarrels   Unlimited Caring   Unlimited Love   Two Siblings Diwali  Celebration'"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_post_str = remove_punctuation(trans_post_str)\n",
    "trans_post_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: \n",
      "['Writting', 'story', 'online', 'for', 'MAR', 'activity', 'Mukhevat', 'Grandpa', 'calls', 'Dadaan', 'Gold', 'Thammi', 'calls', 'Sri', 'Mom', 'Papers', 'Babo', 'I', 'See', 'more', 'Technical', 'Review', 'By', 'Animesh', 'Paloi', 'HAPPY', 'INDEPENDENT', 'DAY', 'I', 'LOVE', 'MY', 'INDIA', 'Up', 'in', 'the', 'hills', 'travelling', 'hikingadventures', 'Unlimited', 'Quarrels', 'Unlimited', 'Caring', 'Unlimited', 'Love', 'Two', 'Siblings', 'Diwali', 'Celebration']\n"
     ]
    }
   ],
   "source": [
    "posts_token_list = nltk.word_tokenize(trans_post_str)\n",
    "print('Tokens: ')\n",
    "print(posts_token_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removal of non ascii words: \n",
      "['Writting', 'story', 'online', 'for', 'MAR', 'activity', 'Mukhevat', 'Grandpa', 'calls', 'Dadaan', 'Gold', 'Thammi', 'calls', 'Sri', 'Mom', 'Papers', 'Babo', 'I', 'See', 'more', 'Technical', 'Review', 'By', 'Animesh', 'Paloi', 'HAPPY', 'INDEPENDENT', 'DAY', 'I', 'LOVE', 'MY', 'INDIA', 'Up', 'in', 'the', 'hills', 'travelling', 'hikingadventures', 'Unlimited', 'Quarrels', 'Unlimited', 'Caring', 'Unlimited', 'Love', 'Two', 'Siblings', 'Diwali', 'Celebration']\n"
     ]
    }
   ],
   "source": [
    "posts_token_list = remove_non_ascii(posts_token_list)\n",
    "print('After removal of non ascii words: ')\n",
    "print(posts_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After convertion to lowercase: \n",
      "['writting', 'story', 'online', 'for', 'mar', 'activity', 'mukhevat', 'grandpa', 'calls', 'dadaan', 'gold', 'thammi', 'calls', 'sri', 'mom', 'papers', 'babo', 'i', 'see', 'more', 'technical', 'review', 'by', 'animesh', 'paloi', 'happy', 'independent', 'day', 'i', 'love', 'my', 'india', 'up', 'in', 'the', 'hills', 'travelling', 'hikingadventures', 'unlimited', 'quarrels', 'unlimited', 'caring', 'unlimited', 'love', 'two', 'siblings', 'diwali', 'celebration']\n"
     ]
    }
   ],
   "source": [
    "posts_token_list = to_lowercase(posts_token_list)\n",
    "print('After convertion to lowercase: ')\n",
    "print(posts_token_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing alphanumeric: \n",
      "['writting', 'story', 'online', 'for', 'mar', 'activity', 'mukhevat', 'grandpa', 'calls', 'dadaan', 'gold', 'thammi', 'calls', 'sri', 'mom', 'papers', 'babo', 'i', 'see', 'more', 'technical', 'review', 'by', 'animesh', 'paloi', 'happy', 'independent', 'day', 'i', 'love', 'my', 'india', 'up', 'in', 'the', 'hills', 'travelling', 'hikingadventures', 'unlimited', 'quarrels', 'unlimited', 'caring', 'unlimited', 'love', 'two', 'siblings', 'diwali', 'celebration']\n"
     ]
    }
   ],
   "source": [
    "posts_token_list = remove_alphanumeric(posts_token_list)\n",
    "print('After removing alphanumeric: ')\n",
    "print(posts_token_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing numbers: \n",
      "['writting', 'story', 'online', 'for', 'mar', 'activity', 'mukhevat', 'grandpa', 'calls', 'dadaan', 'gold', 'thammi', 'calls', 'sri', 'mom', 'papers', 'babo', 'i', 'see', 'more', 'technical', 'review', 'by', 'animesh', 'paloi', 'happy', 'independent', 'day', 'i', 'love', 'my', 'india', 'up', 'in', 'the', 'hills', 'travelling', 'hikingadventures', 'unlimited', 'quarrels', 'unlimited', 'caring', 'unlimited', 'love', 'two', 'siblings', 'diwali', 'celebration']\n"
     ]
    }
   ],
   "source": [
    "posts_token_list = remove_numbers(posts_token_list)\n",
    "print('After removing numbers: ')\n",
    "print(posts_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removal of stopwords: \n",
      "['writting', 'story', 'online', 'mar', 'activity', 'mukhevat', 'grandpa', 'calls', 'dadaan', 'gold', 'thammi', 'calls', 'sri', 'mom', 'papers', 'babo', 'see', 'technical', 'review', 'animesh', 'paloi', 'happy', 'independent', 'day', 'love', 'india', 'hills', 'travelling', 'hikingadventures', 'unlimited', 'quarrels', 'unlimited', 'caring', 'unlimited', 'love', 'two', 'siblings', 'diwali', 'celebration']\n"
     ]
    }
   ],
   "source": [
    "posts_token_list = remove_stopwords(posts_token_list)\n",
    "print('After removal of stopwords: ')\n",
    "print(posts_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After lemmatizing verbs: \n",
      "['writting', 'story', 'online', 'mar', 'activity', 'mukhevat', 'grandpa', 'call', 'dadaan', 'gold', 'thammi', 'call', 'sri', 'mom', 'paper', 'babo', 'see', 'technical', 'review', 'animesh', 'paloi', 'happy', 'independent', 'day', 'love', 'india', 'hill', 'travel', 'hikingadventures', 'unlimited', 'quarrel', 'unlimited', 'care', 'unlimited', 'love', 'two', 'siblings', 'diwali', 'celebration']\n"
     ]
    }
   ],
   "source": [
    "posts_token_list = lemmatize_verbs(posts_token_list)\n",
    "print('After lemmatizing verbs: ')\n",
    "print(posts_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSED POST: \n",
      "writting story online mar activity mukhevat grandpa call dadaan gold thammi call sri mom paper babo see technical review animesh paloi happy independent day love india hill travel hikingadventures unlimited quarrel unlimited care unlimited love two siblings diwali celebration\n"
     ]
    }
   ],
   "source": [
    "p = ' '.join(posts_token_list)\n",
    "preprocessed_post_sentence=\"\"\n",
    "preprocessed_post_sentence=preprocessed_post_sentence+\" \"+p\n",
    "preprocessed_post_sentence = re.sub(r'\\s+', ' ', preprocessed_post_sentence)\n",
    "preprocessed_post_sentence = preprocessed_post_sentence.strip()\n",
    "print(\"PREPROCESSED POST: \")\n",
    "print(preprocessed_post_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('Final_Curated_Dataset1.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    data = list(csvreader)\n",
    "\n",
    "#row_index = 39\n",
    "# Update the value in memory\n",
    "data[row_index - 1][22] = preprocessed_post_sentence\n",
    "\n",
    "# Write the modified data back to the CSV file\n",
    "with open('Final_Curated_Dataset1.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerows(data)\n",
    "#print(data)\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mui Papun', 'Nice', 'So nice', 'Tike has', 'Very smart', 'super pic'}"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trans_comment_str=' '.join(ts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice super pic Mui Papun Very smart Tike has So nice'"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_comment_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice super pic Mui Papun Very smart Tike has So nice'"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_comment_str=remove_URL(trans_comment_str)\n",
    "trans_comment_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice super pic Mui Papun Very smart Tike has So nice'"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_comment_str=replace_contractions(trans_comment_str)\n",
    "trans_comment_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice super pic Mui Papun Very smart Tike has So nice'"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_comment_str = remove_punctuation(trans_comment_str)\n",
    "trans_comment_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: \n",
      "['Nice', 'super', 'pic', 'Mui', 'Papun', 'Very', 'smart', 'Tike', 'has', 'So', 'nice']\n"
     ]
    }
   ],
   "source": [
    "comments_token_list = nltk.word_tokenize(trans_comment_str)\n",
    "print('Tokens: ')\n",
    "print(comments_token_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removal of non ascii words: \n",
      "['Nice', 'super', 'pic', 'Mui', 'Papun', 'Very', 'smart', 'Tike', 'has', 'So', 'nice']\n"
     ]
    }
   ],
   "source": [
    "comments_token_list = remove_non_ascii(comments_token_list)\n",
    "print('After removal of non ascii words: ')\n",
    "print(comments_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After convertion to lowercase: \n",
      "['nice', 'super', 'pic', 'mui', 'papun', 'very', 'smart', 'tike', 'has', 'so', 'nice']\n"
     ]
    }
   ],
   "source": [
    "comments_token_list = to_lowercase(comments_token_list)\n",
    "print('After convertion to lowercase: ')\n",
    "print(comments_token_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing alphanumeric: \n",
      "['nice', 'super', 'pic', 'mui', 'papun', 'very', 'smart', 'tike', 'has', 'so', 'nice']\n"
     ]
    }
   ],
   "source": [
    "comments_token_list = remove_alphanumeric(comments_token_list)\n",
    "print('After removing alphanumeric: ')\n",
    "print(comments_token_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing numbers: \n",
      "['nice', 'super', 'pic', 'mui', 'papun', 'very', 'smart', 'tike', 'has', 'so', 'nice']\n"
     ]
    }
   ],
   "source": [
    "comments_token_list = remove_numbers(comments_token_list)\n",
    "print('After removing numbers: ')\n",
    "print(comments_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removal of stopwords: \n",
      "['nice', 'super', 'pic', 'mui', 'papun', 'smart', 'tike', 'nice']\n"
     ]
    }
   ],
   "source": [
    "comments_token_list = remove_stopwords(comments_token_list)\n",
    "print('After removal of stopwords: ')\n",
    "print(comments_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After lemmatizing verbs: \n",
      "['nice', 'super', 'pic', 'mui', 'papun', 'smart', 'tike', 'nice']\n"
     ]
    }
   ],
   "source": [
    "comments_token_list = lemmatize_verbs(comments_token_list)\n",
    "print('After lemmatizing verbs: ')\n",
    "print(comments_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSED COMMENT: \n",
      "nice super pic mui papun smart tike nice\n"
     ]
    }
   ],
   "source": [
    "preprocessed_comment_sentence=''\n",
    "c = ' '.join(comments_token_list)\n",
    "preprocessed_comment_sentence=preprocessed_comment_sentence+\" \"+c\n",
    "preprocessed_comment_sentence = re.sub(r'\\s+', ' ', preprocessed_comment_sentence)\n",
    "preprocessed_comment_sentence = preprocessed_comment_sentence.strip()\n",
    "print(\"PREPROCESSED COMMENT: \")\n",
    "print(preprocessed_comment_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('Final_Curated_Dataset1.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    data = list(csvreader)\n",
    "\n",
    "#row_index = 39\n",
    "# Update the value in memory\n",
    "data[row_index - 1][24] = preprocessed_comment_sentence\n",
    "\n",
    "# Write the modified data back to the CSV file\n",
    "with open('Final_Curated_Dataset1.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerows(data)\n",
    "#print(data)\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
